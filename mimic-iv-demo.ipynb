{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055e570d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Introduction to the MIMIC-IV Database and Machine Learning with scikit-learn\n",
    "\n",
    "**Author: Meghan R. Hutch**\n",
    "\n",
    "**Date: January 30th, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c9875-a894-4b15-a77e-46e726715880",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### **About Me**\n",
    "\n",
    "* Current Preceptor of Data Science at the University of Chicago\n",
    "* Past PhD Candidate the Luo Lab \n",
    "* Dissertation focused on using EHR data to develop predictive models for patients with neurological illness\n",
    "* Graduated summer 2024 from DGP, Biomedical Informatics Concentration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f28c72-1ad4-407d-980a-4036d1d68d20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### **Today's lecture will be divided into two parts:**\n",
    "\n",
    "* Introduction to working with the MIMIC-IV dataset\n",
    "\n",
    "* Introduction to machine learning with scikit learn\n",
    "\n",
    "\n",
    "We will discuss both of these in the context of a framework for conducting clinical research\n",
    "\n",
    "**Inspired and adapted from Dr. Garrett Eickelberg's [workshop on MIMIC-III and scikit learn](https://github.com/geickelb/MIMIC-III_to_Model/tree/master)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c22f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Framework for Conducting Clinical Research \n",
    "\n",
    "---\n",
    "\n",
    "1. Research Question Specification (hypothesis)\n",
    "<br></br>\n",
    "2. Cohort Specification\n",
    "<br></br>\n",
    "3. Data Extraction\n",
    "<br></br>\n",
    "4. Data Pre-processing (cleaning!)\n",
    "<br></br>\n",
    "5. Model Training\n",
    "<br></br>\n",
    "5. Model Evaluation\n",
    "<br></br>\n",
    "\n",
    "**Today, we will largely focus on steps 3-6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529e06d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Introducing the MIMIC-IV Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7ce6b-f64e-4cda-bd4c-424edb9b4e80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* The Medical Information Mart for Intensive Care (MIMIC)-IV is a large dataset curated to help support studies on intensive care unit (ICU) patients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624083fd-dfde-44e9-a0b5-331a4825a459",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* The MIMIC-IV dataset (since April 2024) contains structured EHR data from >360,000 patients who were admitted to Beth Israel Deaconess Medical Center in Boston, MA between 2008-2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad515fd-7d4e-4e26-b6d9-9238647ee573",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Today, we will use a previously curated [MIMIC-IV demo-dataset](https://physionet.org/content/mimic-iv-demo/2.2/) to introduce how EHR data is computationally structured and how it can be used for research purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d761e39-ce03-4964-9b26-0aa020e4be30",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Notablly, this demo-dataset contains the data from a random subset of 100 hospitalized patients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94552f0a-61f0-4337-9f2d-9a702e48d931",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* This dataset was curated for the purposes of workshops and tutorials. Thus, having appropriate CITI training is not needed. Although deidentified, access to the full dataset requires a physionet account and [CITI training](https://physionet.org/about/citi-course/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1578d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data Extraction\n",
    "\n",
    "- The full demo dataset has been downloaded to the classroom's quest allocation.\n",
    "- The data used for this lecture are available in the relevant [GitHub Repo](https://github.com/meghutch/mimic-ml-demo)\n",
    "- We will further load this data into our notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9e4be5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Load in MIMIC-IV Demo\n",
    "\n",
    "First, we will import some basic packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23991120",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import our packages\n",
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9914d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set our working directory - this notebook assume you are in the classroom Quest folder ('/projects/e30766/mimic-iv-demo/2.2') where the MIMIC-IV demo dataset is located\n",
    "#os.chdir('/projects/e30766/mimic-iv-demo/2.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758127d-ca7c-4637-9ca2-661ddda4eaee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set relative working directory assuming you downloaded this lecture from GitHub:\n",
    "os.chdir('data/mimic-iv-clinical-database-demo-2.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb5b820",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make sure you are in the working directory: \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af8727-79e7-40a9-9031-1adf291cf848",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Next, we will load the list of subject identifiers (`subject_id`) for the demo cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064c9af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "subject_id = pd.read_csv('demo_subject_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b15fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the first 5 rows of data\n",
    "subject_id.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0131df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check how many unique subjects\n",
    "subject_id['subject_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0941b1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Overview of MIMIC-IV Tables**\n",
    "\n",
    "For this demo, we will explore data found in the following two MIMIC-IV modules:\n",
    "\n",
    "\n",
    "### [Hosp](https://mimic.mit.edu/docs/iv/modules/hosp/)\n",
    "\n",
    "> \"The Hosp module provides all data acquired from the hospital wide electronic health record. Information covered includes patient and admission information, laboratory measurements, microbiology, medication administration, and billed diagnoses.\"\n",
    "\n",
    "### [ICU](https://mimic.mit.edu/docs/iv/modules/icu/)\n",
    "\n",
    "> \"The ICU module contains information collected from the clinical information system used within the ICU. Documented data includes intravenous administrations, ventilator settings, and other charted items.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfe198-ae72-4b56-88f2-7e644e628882",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Notes:**\n",
    "\n",
    "* The [GitHub repo](https://github.com/meghutch/mimic-ml-demo/tree/main) for this lecture only contains the data used for this workshop (due to GitHub storage limits). However, all data tables within the `hosp` and `icu` modules have been downloaded to Quest: `/projects/e30766/mimic-iv-demo/2.2`\n",
    "\n",
    "* The full MIMIC-IV dataset also contains additional modules for data from emergency department (ED), chest X-rays (CXR), and clinical notes (Note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f72ef-429e-4bf2-bb0b-e479c9668462",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Next, let's look at the `patient` table found within the `hosp` module.\n",
    "\n",
    "The `patient` table contains each patient's unique `subject_id` and some simple demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68d33d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "patients = pd.read_csv('hosp/patients.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b015ec8-eb6b-461e-824d-fba012cad7d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review the first 5 rows\n",
    "patients.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc4f80e-6c13-4856-b862-0e9f3c232998",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "What do these columns mean? \n",
    "\n",
    "For this, we can review the data dictionary: https://mimic.mit.edu/docs/iv/modules/hosp/patients/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7401f6-e814-44a0-8192-eb3894ad42f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review the first 5 rows\n",
    "patients.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22091455",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Important note about MIMIC's pre-processing of [dates](https://mimic.mit.edu/docs/iv/modules/hosp/patients/#anchor_age-anchor_year-anchor_year_group):\n",
    "\n",
    "* Dates are randomly shifted consistently for all patients\n",
    "\n",
    "* `anchor_year` - shifted year for the patient\n",
    "\n",
    "* `anchor_year_group` -  is a range of years - the patient’s `anchor_year` actually occurred during this range\n",
    "\n",
    "* `anchor_age` - the patient’s age in the `anchor_year` (year of admission). \n",
    "\n",
    "**Examples**: \n",
    "\n",
    "A patient has an `anchor_year` of 2153, `anchor_year_group` of 2008 - 2010, and an `anchor_age` of 60.\n",
    "\n",
    "* The year 2153 for the patient corresponds to 2008, 2009, or 2010.\n",
    "\n",
    "* The patient was 60 in the shifted year of 2153, i.e. they were 60 in 2008, 2009, or 2010.\n",
    "\n",
    "* A patient admission in 2154 will occur in 2009-2011, an admission in 2155 will occur in 2010-2012, and so on.\n",
    "\n",
    "* Note: Due to HIPPA patient confidentiality concerns, if a patient’s `anchor_age` is over 89 in the `anchor_year` then their `anchor_age` is set to 91, regardless of how old they actually were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140b75f-61c8-4c4f-8180-99a79b21b5c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review the first 5 rows\n",
    "patients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb7891",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review the number of unique values in each dataset\n",
    "patients['subject_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78ad6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf294e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Let's load in several additional tables from MIMIC:**\n",
    "\n",
    "Here, we will load a few of the tables we will use to demonstate the workflow of aquiring, pre-processing, and analyzing data from the MIMIC database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a6a73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load several tables from the hosp module\n",
    "admissions = pd.read_csv('hosp/admissions.csv.gz') # info about hospital stay\n",
    "omr = pd.read_csv('hosp/omr.csv.gz') # misc info from EHR\n",
    "labevents = pd.read_csv('hosp/labevents.csv.gz') # laboratory measurements\n",
    "d_labitems = pd.read_csv('hosp/d_labitems.csv.gz') # dictionary of laboratory values\n",
    "diagnoses = pd.read_csv('hosp/diagnoses_icd.csv.gz') # icd codes assigned to patient for billing\n",
    "d_icd_diagnoses = pd.read_csv('hosp/d_icd_diagnoses.csv.gz') # dictionary of icd variable\n",
    "\n",
    "# load several tables from the icu module\n",
    "chartevents = pd.read_csv('icu/chartevents.csv.gz') # info charted during patient's hospital stay\n",
    "d_items = pd.read_csv('icu/d_items.csv.gz') # dictionary of ICU event variables  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c13328",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### omr table\n",
    "\n",
    "> \"The Online Medical Record (OMR) table contains miscellaneous information from the EHR.\"\n",
    "\n",
    "[omr documentation](https://mimic.mit.edu/docs/iv/modules/hosp/omr) of columns:\n",
    "\n",
    "* `chartdate` - the date on which the observation was recorded\n",
    "* `seq_num`- a monotonically increasing integer which uniquely distinguishes results of the same type recorded on the same day. For example, if two blood pressure measurements occur on the same day, seq_num orders them chronologically.\n",
    "* `result_name` - human interpretable description of the observation\n",
    "* `result_value` - the avalue associated with the observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b6c6e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review the first 5 rows\n",
    "omr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5dc8f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review unique result_names (observations)\n",
    "# here we can see the unique variables/measurements contained in this table\n",
    "omr[['result_name']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b308bff-5eb6-4bd1-80fb-a6345b79bad2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data Pre-processing (Data Cleaning)\n",
    "\n",
    "### What does it mean to clean data?\n",
    "\n",
    "Simply, data cleaning is the act of preparing data for analysis. In a clinical context, we also want to make sure that the data we are using is clinically relevant and accurate, especially in regard to the specific clinical question or problem we are trying to solve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f4330",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### How do we \"clean\" data:\n",
    "\n",
    "* Ensure data are formatted into the right data types (numeric, character, date, etc)\n",
    "\n",
    "* Review summary statistics (how many observations, how many patients have each observation, mean/median/sd/variance of values, proportion of missing data)\n",
    "\n",
    "* Review the distribution of data\n",
    "\n",
    "* Are there possible data entry problems or unit conversions needed?\n",
    "\n",
    "* How should we handle missing data?\n",
    "\n",
    "* Should patients have repeated measurements (e.g. multiple white blood cell counts on the same day)? If so, how should we handle repeated observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd17f08-494b-4db5-9f92-ae857d7ae393",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "> “Data science, he says, involves multiple “very small decisions” — data cleaning and filtering steps, for instance, which are crucially important, but difficult to document. And journal page limits preclude exposition. But by blending code, data and text in a single document, researchers can show just how their results were generated.” - Ben Marwick; [Perkel, JM](https://www.nature.com/articles/d41586-022-00563-z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a63a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Example of Data Cleaning (Height)\n",
    "\n",
    "**Let's focus on the variables `Height` and `Weight`**\n",
    "\n",
    "We will filter the `omr` table to only contains results for height and weight. During data cleaning, each variable will be saved in its own dataframe for ease of pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb1122",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create separate dataframes for each variable\n",
    "height = omr[omr['result_name']==\"Height (Inches)\"].copy() # copy tells Python to keep our new df 'height' independent from the original df 'omr' it was sourced from\n",
    "\n",
    "weight = omr[omr['result_name']==\"Weight (Lbs)\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db2b8ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Examine Height (inches)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578015b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f573ee9-a122-4b21-94a4-374022a9a296",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's check the number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569094e8-a04b-456b-9e4e-5e9357c0c9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of height measurements:', len(height))\n",
    "print('Number of unique patients with height measurements:', height['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7bae6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "It looks like `result_value` is a continuous variable. Let's make sure that Python also has recognized it as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6482b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check variable data types\n",
    "height.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b1b48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "It looks like Python read `result_value` as a object (or character) variable. Let's convert to numeric. This is an important step to make sure any downstream functions (calculating the mean or median, plotting the distribution, etc) work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac6731",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height['result_value'] = pd.to_numeric(height['result_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73725a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check variable data types again to make sure our transformation and code worked\n",
    "# float64 indicates a continuous variable, so this looks good!\n",
    "height.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d9236e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Using Summary Statistics and Visualization to help Evaluate Data\n",
    "\n",
    "Next, let's calculate summary statistics to get a better sense of our variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c506f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height['result_value'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8974f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Visualize distribution of values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee820d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(height['result_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bccaf75",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Potential red flag:** Is there really a patient who might be 5 inches tall? This is a clear case I'd like to investigate more. Perhaps this is a data entry issue or a newborn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef95b9c-a55b-434a-8aae-3fb059f38377",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Further explore data to check for inconsistencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807f9cc-4925-4501-a28d-2c99f980693d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "First, we can sort the height values from lowest to highest in order to retrieve the `subject_id` of the patient with a height of 5 inches\n",
    "\n",
    "I can also see if there are any other patients with unusually low heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f9519-8d9f-4b4a-a053-b6517cda6482",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height.sort_values(['result_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86cf85-0390-4eb4-818f-55e79d9c341e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's see if this patient has other recorded height measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd8904",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's see if this patient has other recorded height measurements\n",
    "height[height['subject_id']==10012853].sort_values('chartdate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059cd240",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Investigate patients with multiple observations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f821b3-272e-4075-a00d-e8c035fc07d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For each unique patient, we will calculate the standard deviation of the patient's height measurements\n",
    "\n",
    "*Note*: `ddof = 0` indicates that std for patients with one measurement will be displayed as 0 rather than NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258a3aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height['std'] = height.groupby('subject_id')['result_value'].transform(np.std, ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4b0d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height.sort_values('std', ascending = False)[['subject_id', 'std']].drop_duplicates().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571be392-c5cf-427f-a3e6-4888c9480fd9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can clearly see the one 'concerning' patient has a much higher standard deviation of their height measurements compared to other patients.\n",
    "\n",
    "Let's take a look at patient with the second highest standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe650c0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height[height['subject_id']==10005909].sort_values('chartdate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d26544-8aed-4284-b1ca-b5e646ad4d96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**For patients with multiple observations, let's calculate the median**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544a024-3b30-4fa3-b13e-29b511783f26",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new variable with the median height value for each patient\n",
    "height['result_value_median'] = height.groupby('subject_id')['result_value'].transform('median')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221a6f4-7766-40d6-9b63-b4c5e43565e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Save the new dataset**\n",
    "\n",
    "This next step ensures that we will now only have one unique row per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29609a8-630c-4273-8b2c-bb5ce3a558c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assuming we don't need to link by date\n",
    "height_clean = height[['subject_id', 'result_name', 'result_value_median']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d977b8-0180-4697-84bf-1231e9d62eba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's review the distribution again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fa636-c409-4ce7-8f69-55a63318232a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(height_clean['result_value_median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f27d2-a946-43fa-93e7-3fb0fa2d59ed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the number of unique subject_id should match our original height dataframe\n",
    "height_clean['subject_id'].nunique() == height['subject_id'].nunique()\n",
    "\n",
    "# important to check to make sure we didn't accidently lose information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634f4ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Important Notes:** \n",
    "\n",
    "* The best way to handle these types of inconsistencies will largely depend on the exact variable, question of interest, and domain knowledge. \n",
    "<br> </br>\n",
    "* Seek consultation from mentors and domain experts! \n",
    "<br> </br>\n",
    "* In this case, I'm not considering the date or specific admission. Some patients have multiple admissions. \n",
    "<br> </br>\n",
    "* Additionally, depending how old they are, height may be expected to change over time. In those cases, we might want to consider the patient's age before just taking the median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792882fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Another Example of Data Cleaning (Weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbdf0a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c3d57",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Review variable types**\n",
    "\n",
    "It looks like `result_value` is a continuous variable. Let's make sure that Python also has recognized it as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db656c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check variable data types\n",
    "weight.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c76423",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight['result_value'] = pd.to_numeric(weight['result_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb7d22",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check variable data types\n",
    "weight.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8487aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight['result_value'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094cd6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Distribution of weight values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a72ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(weight['result_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e64d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Investigate the standard deviation in the measurements of patients with multiple observations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b44176",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight['std'] = weight.groupby('subject_id')['result_value'].transform(np.std, ddof=0)# ddof = 0 indicates that std for patients with one measurement will be displayed as 0 rather than NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef71ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight.sort_values('std', ascending = False)[['subject_id', 'std']].drop_duplicates().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7493dee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight[weight['subject_id']==10019385].sort_values('chartdate') # did this patient lose > 110 pounds in < 1 month?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53af5d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Potential Unit Issue?**\n",
    "\n",
    "If 1 kg = 2.20462 lbs:\n",
    "\n",
    "97.30 kg * 2.20462 = 214.5 lbs (this is almost exactly what was measured in the first observation!)\n",
    "\n",
    "This is a case where I'd feel comfortable manually changing a value - we can feel fairly confident that the 97.3 was measured in kgs instead of lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc27354-3872-442f-9792-788ed0a2968e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight['result_value'] = np.where((weight['subject_id'] == 10019385) & (weight['result_value'] < 100), weight['result_value']*2.20462, weight['result_value']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d04c1-6b5e-4326-8380-3d5e60948594",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight[weight['subject_id']==10019385].sort_values('chartdate') # did this patient lose > 110 pounds in < 1 month?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b21d88-0fc6-4c0c-baaa-20c812c3bfe4",
   "metadata": {},
   "source": [
    "Review a second patient with high standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e92a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight[weight['subject_id']==10021487].sort_values('chartdate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ea010",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Notes:\n",
    "\n",
    "In the first case, my intuition is that the discrepancy in weight measurements is due to a unit conversions issue\n",
    "\n",
    "In the second case, the patient has many weight measurements over the a 1.5 year period. The measurements themselves don't vary too widely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7e80c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Missing Data\n",
    "\n",
    "This topic could be it's own class. \n",
    "\n",
    "Rule of thumb: Talk to your domain experts/collaborators!\n",
    "\n",
    "Some common ways to handle missing data:\n",
    "\n",
    "* Remove those patients\n",
    "* Impute mean or median\n",
    "* Forward/backward filling\n",
    "* MICE\n",
    "\n",
    "In the above cases, I used the median to impute. This may or may not be the best method depending on your use cases. Here, I wanted to keep it simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ecd716",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Evaluating Unit Conversions\n",
    "\n",
    "Another clinical variable that often needs cleaning are laboratory test measurements.\n",
    "\n",
    "In MIMIC we can use the `labevents` and `d_labitems` tables to work with laboratory data. \n",
    "\n",
    "Importantly, `d_labitems` links to the `labevents` table. \n",
    "\n",
    "The `d_labitems` provides the definitions (or names) or the specific labs via the `itemid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbe6cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's first look at the labevents table\n",
    "labevents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cdc50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's first look at the d_labitems table\n",
    "d_labitems.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d5c5e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Example Data Cleaning (Laboratory Tests)\n",
    "\n",
    "1. We can merge `d_labitems` and `labevents` by shared identifiers (we'll review this in the next section on merging tables)\n",
    "\n",
    "2. We can search for specific labs of interest via the `label` column\n",
    "\n",
    "3. We can also filter with specific `itemid` numbers if we know which correspond to labs of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be92774",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "First, let's try and see whether we can find the `itemid` for potassium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061fbc9c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_labitems[d_labitems['label'].str.contains('potassium', case = False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15eb0d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "potassium = labevents[labevents['itemid']==50971]\n",
    "potassium.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb659c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "potassium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771e36a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(potassium['valuenum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c0143",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's check the distinct units of potassium\n",
    "potassium[['valueuom']].value_counts() # only unit is Milliequivalents per liter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e236f58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Let's evaluate another lab test, one which has more than one unit specification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318115e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_labitems[d_labitems['itemid']==51249]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a9eba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *Mean Corpuscular Hemoglobin Concentration\n",
    "MCHC = labevents[(labevents['itemid']==51249)]\n",
    "MCHC['valueuom'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a4a9c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lab_perc = MCHC[MCHC['valueuom']=='%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215fdf6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lab_gdl = MCHC[MCHC['valueuom']=='g/dL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccde471",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(lab_gdl['valuenum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b71aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(lab_perc['valuenum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6564f7-c0a9-4a1d-947f-1a285a074730",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Cleaning EHR Data - Common pitfalls :\n",
    "\n",
    "- missing data\n",
    "- inaccurate data\n",
    "- data inconsistencies\n",
    "- misunderstanding data and where/how it was generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e44546-16d6-45bf-9f5e-cc985349d88d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Working with large EHR datasets\n",
    "\n",
    "#### Merging tables \n",
    "\n",
    "Tables can be merged (e.g. linked) together through shared identifiers. The MIMIC official [documentation](https://mimic.mit.edu/docs/iv/) is a great reference for learning more about the associations between tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a735950-9ed8-4cf2-9686-91a2ff1fd327",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "First, let's take a look at the diagnosis data that we have for patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6234c1-2e9d-4975-b5e6-c2108cbf347f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diagnoses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f3f43-1861-49b4-94c4-45f471f38366",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "In order to obtain more detailed information about each ICD code, we can use the the `d_icd_diagnoses` which contains a longer description of each icd code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b2f8e-19a7-4af2-8a06-760e099db849",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_icd_diagnoses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025bcb3-68b3-4a90-b0db-50437e4cc07e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This table can be linked to the `diagnoses` dataframe using the `icd_code` and `icd_version` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13daa5ce-8e0c-40a5-9c15-d8abed7d4712",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diagnoses_with_code = pd.merge(diagnoses, \n",
    "                               d_icd_diagnoses,\n",
    "                               on = ['icd_code', 'icd_version'],\n",
    "                               how = 'inner')\n",
    "\n",
    "diagnoses_with_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae078234-705f-4dec-812d-f1b5c367617c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's count the most frequent diagnoses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975455b-cdc3-4298-b5bc-a49c2665332b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diagnoses_with_code[['long_title']].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6252a4-9da1-4071-b067-bbbd11b92193",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Similarly, we can merge the diagnostic data with our `admissions` dataframe in order to aquire more information about the hospital stays of these patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3670c-e783-4991-a602-d81e32fa7464",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "admissions_with_icd = pd.merge(admissions,\n",
    "                               diagnoses_with_code,\n",
    "                               on = ['subject_id', 'hadm_id'],\n",
    "                               how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085d049-348f-49bb-9d1d-1843c34a1d77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "admissions_with_icd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d0680-3514-4d6b-a76e-5a0a4629ab51",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "What if we are interested in a specific disease, but did not know the associated ICD code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dcc8e5-4ad0-4926-9468-279b1081e348",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Filtering datasets (cohort specification)\n",
    "\n",
    "Let's say we are only interested in patients with sepsis. We could filter our dataset using keyword matching or specific ICD codes (if we have specific codes we are interested in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c92b8-2a49-4efd-9da2-efe244bbb7ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sepsis_cohort = admissions_with_icd[admissions_with_icd['long_title'].str.contains('sepsis', case = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a2661-a8fe-4f0e-b822-0011f21f9d58",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sepsis_cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd6a46-f45d-49cd-bce6-fa2a6329bf53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sepsis_cohort.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdabb213-784b-4078-bcfb-136e3a182751",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sepsis_cohort[['icd_code', 'icd_version', 'long_title']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb09369-2089-405d-91bb-f0cddb8a577a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sepsis_cohort_icd = admissions_with_icd[admissions_with_icd['icd_code']=='99592']\n",
    "sepsis_cohort_icd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb99c9-5fc2-44ff-b84f-d23500180304",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Introduction to Machine Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462dba09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Case Study: Prediction of ICU Mortality with Supervised Learning\n",
    "\n",
    "For this demonstration, we will use a previously prepared dataset from an earlier version of MIMIC. This data comes from a [community challenge](https://physionet.org/content/challenge-2012/1.0.0/) focused on predicting mortality with the MIMIC dataset.\n",
    "\n",
    "We will use the previously pre-processed dataset from one of the challenge winners Alistair Johnson (https://github.com/alistairewj/challenge2012) - so we won't do much of our data cleaning this time (though it is still good practice!). Johnson also includes his [pre-processing notebook](https://github.com/alistairewj/challenge2012/blob/master/prepare-data.ipynb) which can provide another example of how to proceed with cleaning clinical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abe321-e4a3-4451-b0c5-3b343abae845",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "First, we will load in our machine learning functions from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdff6f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in our machine learning functions from scikit-learn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, roc_auc_score, precision_score, recall_score, accuracy_score, auc, average_precision_score, precision_recall_curve, precision_recall_fscore_support, f1_score, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV,RandomizedSearchCV,cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d7cdd-c67c-4d03-8e91-9b646a6b0335",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In this dataset, we will train a machine learning model to predict mortality using 4,000 unique patient records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b609a367",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "# load in pre-processed challenge dataset\n",
    "train = pd.read_csv('../challenge2012/data/PhysionetChallenge2012-set-a.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71062d7d-5892-49c1-98ee-00d0536e5342",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This data has already been pre-processed but it is good practice to still examine our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051bd80e-895e-406e-9fca-ed78b189c088",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "First, let's examine the variables with the highest number of missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f160ce-72de-40e9-b7ae-241927a1dcf6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.isnull().sum().sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0fd5b9-5687-458d-a621-febe30abbf45",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's create a separate dataframe to tally the number of missing values and plot the proportion of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ce71e-19c3-494b-82d4-2aea66c885e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the proportion of missingness\n",
    "num_missing = train.isnull().sum()\n",
    "num_missing =  num_missing/len(train)\n",
    "\n",
    "num_missing.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111f5a1-1bf0-4ed8-bd4a-628a7400bbfd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Determine which features to keep in our model**\n",
    "\n",
    "We can see that there are many variables that are largely incomplete. For our purposes, let's first only keep those variables which have <= 25% missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143bfb4-3edd-44a1-abaf-cc5b1110cd18",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vars_low_missing_rate = num_missing[num_missing <= 0.25].index.tolist()\n",
    "\n",
    "len(vars_low_missing_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ae36f-2ae0-47aa-ba39-0d5820c5628a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Using the list of columns with a low missing rate, we can subset our `train` data to only include these columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0ff507-362d-4613-b313-7c19d4855182",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep columns with a low missing rate\n",
    "X_vars = train[vars_low_missing_rate]\n",
    "\n",
    "X_vars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905a8e64-8a4e-4a33-9d50-9f346f141c0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We can further reduce the number of variables by only choosing the first lab or vital measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9a2ea-9ca4-4204-9aa6-77fe2c243195",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_first = X_vars.filter(regex='_first$')# keeps columns with the word 'first'\n",
    "X_first.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a9db5f-5cd5-48b5-ac76-6e6826da78af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We should also consider keeping demographic (`Age`, `Gender`) and baseline information like `Weight` or intensive care unit admitted (`CCU`, `CSRU`, `SICU`) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb3317-9e36-476b-a70d-49d99ca9c4c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Keep Initial Demographics and Clinical Information (Type of ICU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f35692-871e-4435-9138-6646c268ec10",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_baseline = X_vars[['Age', 'Gender', 'Weight', 'CCU', 'CSRU', 'SICU']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec144b-d669-4961-8d45-10f4b6387259",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Combine maintained set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267bc9b-2ece-4a22-9afb-1e63a81a7cb1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = pd.merge(X_baseline, \n",
    "                   X_first,\n",
    "                   left_index=True, \n",
    "                   right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0994d2-82ad-4d5c-8178-d775ec809ceb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e05e04-0f65-4208-9009-81cfd005488a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Review number of missing variables once more on the 'cleaner' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd21bd-f595-4681-899e-a915403c8e87",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f19121-5458-4851-a1ff-bcafff99e838",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will drop the 3 patients without a gender\n",
    "X_train = X_train.dropna(subset=['Gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58140154-65bc-46cb-8ca3-9ab8b9247ca0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Create `y` dataframe that will only contain our outcome of interest (`in-hospital_death`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802a332-6e23-4e7d-942d-055de5fdad33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this step takes our `in-hospital_death` column from the original `train` data and combines it with the cleaned up X_train dataframe\n",
    "# this allows us to remove the 3 patients that we dropped who were missing gender\n",
    "y = pd.merge(train['In-hospital_death'], X_train,\n",
    "             left_index=True, \n",
    "             right_index=True)\n",
    "\n",
    "# select outcome of interest\n",
    "y = y['In-hospital_death']\n",
    "\n",
    "# rename y df\n",
    "y_train = y.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b5a83-69e8-45bc-be32-6f3f87a1b6cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Tally the number of patients meeting each outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f6f1e-b2d2-4a2c-9b27-c4450998ea92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400d017-90b5-42d6-8784-a40a8fa090b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### **Training a [Lasso Logistic Regression](https://scikit-learn.org/dev/modules/linear_model.html#binary-case) Model**\n",
    "\n",
    "These next modules will demonstrate the use of a Least Absolute Shrinkage and Selection Operator (LASSO) logistic regression classifier to predict outcomes\n",
    "\n",
    "<img src=\"lecture_images/logistic_regression.png\" alt=\"title\" width=\"400\" align=\"left\">\n",
    "\n",
    "[Image Source](https://www.javatpoint.com/logistic-regression-in-machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6933686-d748-4f3d-a16a-f65475db6ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "In logistic regression, we are trying to predict the probability of $y=1$ given a set of features $X$. \n",
    "\n",
    "In our case, $y$ represents `in-hospital_death`. Thus, $y = in-hospital__death = 1$ represents that the patient died in the hospital\n",
    "\n",
    "For each data point $i$ (or each patient observation):\n",
    "\n",
    "$P(y_i=1|X_i)$\n",
    "\n",
    "In other words, logistic regresion will allow us to predict the probability that the $y$ outcome of a patient is hospital death given the specific set of $X$ features of that patient.\n",
    "\n",
    "**Other Notes**:\n",
    "\n",
    "* Logistic Regeression is used for classification, thus need a binary outcome variable (e.g. survival or death)\n",
    "* Good baseline model (simple!)\n",
    "* L1 (Lasso) vs L2 (Ridge) regularization - a penalty to avoid overfitting - essentially, shrinking the coefficients of non-informative variables\n",
    "* [scikit-learn documentation](https://scikit-learn.org/dev/modules/generated/sklearn.linear_model.LogisticRegression.html#logisticregression) shows us which parameters we can set when constructing a logistic regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c4f9a-c8ed-4550-a55c-3e3403437dbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### **Setting up a Machine Learning Pipeline**\n",
    "\n",
    "Before, we can start training, there is machine learning pre-processing steps we have to take. \n",
    "\n",
    "Fortuntately, scikit-learn has a lot of useful functions (such as `Pipepline()`) to help streamline this process. \n",
    "\n",
    "The code below sets up the pipeline. Essentially, we want our pipeline to do the following:\n",
    "\n",
    "1. Impute any missing values (via the median)\n",
    "2. Standardize numeric variables (mean 0 and standard deviation of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfaa998-9f07-433b-a42d-5f5d51f37625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify numeric features to standardize and impute \n",
    "numeric_features = ['Age', 'Weight', 'GCS_first', 'Glucose_first', 'HR_first', 'NIDiasABP_first', 'NIMAP_first',\n",
    "                    'NISysABP_first', 'Temp_first', 'BUN_first', 'Creatinine_first', 'HCO3_first', 'HCT_first', 'K_first', \n",
    "                    'Mg_first', 'Na_first', 'PaCO2_first', 'PaO2_first', 'Platelets_first', 'WBC_first', 'pH_first']\n",
    "\n",
    "# specify non-numeric features\n",
    "non_numeric_features = ['Gender', 'CCU', 'CSRU', 'SICU']\n",
    "\n",
    "# construct pipeline for preprocessing data \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "non_numeric_transformer = 'passthrough'  # Pass non-numeric features unchanged\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('non_num', non_numeric_transformer, non_numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize lasso (penalty = 'l1') logistic regression model\n",
    "# class_weight = 'balanced' since the classes of our model (death vs no death) is heavily imbalanced\n",
    "lr = LogisticRegression(solver='liblinear', random_state = 12345, penalty = 'l1', class_weight = 'balanced')\n",
    "\n",
    "# Create a pipeline with preprocessing and a model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lr)  # Replace with your model of choice\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf367e9-db7d-4ef5-ba25-2e3c8334b7a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "These pipeline steps make it possible to prepare our data for model training and tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e355a-e220-41f8-8204-dff7c9e2cf8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### What is model tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611fcf7-d5cd-414b-a408-92b817a7e011",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Machine learning models have different types of hyperparamters which can be changed to try and optimize model performance. \n",
    "\n",
    "We won't discuss too much here, but essentially, we often are training a model to minimize an objective function (also referred to as a loss or cost function depending on the context). Tuning these parameters is what can help minimize the model's objective function and improve classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944aad93-466c-43f1-aaa0-054870ec3edd",
   "metadata": {},
   "source": [
    "Additionally, tuning is often performed using **cross-fold validation**. A technique where a dataset is divived into $k$ folds (usually k=5 or k=10). \n",
    "\n",
    "Thus, if $k=5$, we randomly divide the model into 5 equal folds:\n",
    "\n",
    "<img src=\"lecture_images/k_fold_cross_validation.png\" alt=\"title\" width=\"400\" align=\"left\">\n",
    "\n",
    "* 4 folds are used for training with different sets of hyperparameters \n",
    "* The remaining held-out fold is used how the model does on data it hasn't seen before\n",
    "* This process is repeated until each fold is used as the held-out test set once\n",
    "* The optimal hyperparaters are chosen based on the average performance across folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa44501-b8e8-4b0f-95a4-05ab29faae61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### How do we choose and evaluate the different hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d3c24-5d2a-4177-9db1-399b1d290412",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "In this demonstration, we will evaluate hyperparameters using the scikit-learn function `GridSearch` - this allows us to pre-specify the hyperparameters we want to evaluate in a grid. During Cross-fold validation, we we test all possible hyperparameter combinations in this grid to determine those which provide the best performance across folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f58dd-2820-4f7a-9c06-e68a3f50903a",
   "metadata": {},
   "source": [
    "### Cross-validation and GridSearch\n",
    "\n",
    "Let's set up `GridSearchCV` framework to train and tune our model with 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a249bb-ea9d-4bb7-8cbb-2b14eec808c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for model tuning\n",
    "param_grid = {\n",
    "    'classifier__tol': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]   # Range of regularization strengths (inverse of regularization strength)\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "lr_grid_search = GridSearchCV(estimator=model_pipeline, # specific our pipeline as our estimator \n",
    "                              param_grid=param_grid,\n",
    "                              cv=5,\n",
    "                              scoring='roc_auc', # this specifies that we are going to use the AUC as a metric to evaluate how well our model does across folds\n",
    "                              return_train_score=True,\n",
    "                              refit=True,\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0998a-394c-450a-9585-569f3e13e6a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now that we've specified our pipeline and implemented it into our gridsearch/cross validation, we can train (or fit!) our model to our training data. The code will automatically create our folds for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139bacc-7622-4d35-bc01-78361b541a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the random search object to the data\n",
    "fit_lr_grid_search = lr_grid_search.fit(X_train, y_train)\n",
    "fit_lr_grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644a28a-d0c1-4e24-b964-13659b239c17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### So what is going on when we fit this model?\n",
    "\n",
    "Our pipeline ensures the following:\n",
    "\n",
    "1. Our data is divided into 5-folds\n",
    "2. We train the model on 4-folds\n",
    "3. These 4-folds are pre-processed (numeric data standardized and imputed as needed)\n",
    "4. GridSearch tests each combination of hyperparamters (`tol=1e-4` and `C=0.001`; `tol=1e-4` and `C=0.01`; and so forth)\n",
    "5. For each fold and combination of hyperparameters, the model calculates the AUC on the held-out fold\n",
    "6. This process repeats until all 5-folds and hyperparamter combos have been evaluted\n",
    "7. The set of hyperparameters with the highest average AUC will be chosen as our best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37420e2-b616-40f8-ab38-318729b835bb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### **Concept Check:** \n",
    "\n",
    "Why do we perform the pre-processing step on 4-folds at a time? Can we standardize and impute all of the data prior to starting the training/tuning steps? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9295b-e90c-4f91-a128-d55f5d64119d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Model Evaluation on Training set\n",
    "\n",
    "We can determine what the best hyperparameters and the average AUC is from the fitted object `fit_lr_grid_search`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be341d0-42d2-457b-88ba-7628e10bf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters and score\n",
    "print(f\"Best performing hyperparameters: {fit_lr_grid_search.best_params_}\")\n",
    "print(f\"Average AUC: {fit_lr_grid_search.best_score_.round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20dab9f-11e4-4448-a0c5-cfae6a2d8c73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can also see the results from all of the models evaluated during the tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f483431-60d2-4405-ab4d-2ae88b899419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access the cv_results_ dictionary\n",
    "cv_results = fit_lr_grid_search.cv_results_\n",
    "\n",
    "# Create a DataFrame from cv_results_\n",
    "results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a7948-eafe-4221-8109-dc3d78c9a767",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's just look at the best performing model (based on AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bbbc8f-41e1-4c4c-bc65-f9e16758c520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the row corresponding to the best index\n",
    "best_row_df = results_df.loc[[fit_lr_grid_search.best_index_]]  # Use double brackets to keep it as a DataFrame\n",
    "best_row_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18789f9-28f6-44cc-87c2-9dbd98bfcb91",
   "metadata": {},
   "source": [
    "## Training Evaluation\n",
    "\n",
    "There are many ways to evaluate the performance of our training. The below modules will review a few!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160fad7-1d60-4204-9fef-b61e6c5989d4",
   "metadata": {},
   "source": [
    "### Model Coefficients (important predictors)\n",
    "\n",
    "Default coefficients are represented as log-odds\n",
    "\n",
    "We can exponentiate the log-odds to get the odds ratio:\n",
    "\n",
    "$\\text{Odds Ratio} = e^{\\text{coef}}$\n",
    "\n",
    "**To intepret:**\n",
    "* A feature with an odds ratio > 1 - are features in which the odds of death increase when that feature increases\n",
    "* A feature with an odds ratio < 1 - are features in which the odds of death decrease when that feature increases\n",
    "* A feature with an odds ratio = 1 - are features which have no effect on the outcome (death)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf8fd2-1d58-439f-aaeb-0b2885a72975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access the best estimator\n",
    "best_lasso = lr_grid_search.best_estimator_\n",
    "\n",
    "# Extract coefficients from the Lasso model\n",
    "lasso_coefficients = best_lasso.named_steps['classifier'].coef_.flatten().tolist()\n",
    "\n",
    "# If needed, map coefficients to feature names\n",
    "feature_names = X_train.columns.tolist()  # Replace with your list of feature names\n",
    "coefficients_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lasso_coefficients\n",
    "})\n",
    "\n",
    "coefficients_df['abs(coef)'] = coefficients_df['Coefficient'].abs()\n",
    "coefficients_df['Odds Ratio'] = np.exp(coefficients_df['Coefficient'])\n",
    "\n",
    "# Display the coefficients\n",
    "coefficients_df.sort_values(by='Odds Ratio', ascending=False).reset_index(drop=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f7243-2709-4e83-a6c2-0e42cd317bc9",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix helps us calculate our true and false positive rates. This in turn, helps us calculate important metrics such as: \n",
    "\n",
    "* sensitivity - TP / (TP + FN)\n",
    "* specificity - TN / (TN + FP) \n",
    "* positive predictive value (precision) - TP / (TP + FP)\n",
    "* negative predictive value - TN / (TN + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8338b4-0cd0-4e80-a25e-da6159cf8166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted classes using default 0.5 threshold\n",
    "y_hat = fit_lr_grid_search.predict(X_train) \n",
    "\n",
    "print(\"classification report:\\n \", classification_report(y_train, y_hat, digits=3))\n",
    "print(\"confusion matrix:\\n \")\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_train, y_hat)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45e045-8594-46ba-8a5a-e22878751443",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic Curve\n",
    "\n",
    "A ROC plots the true postitve rate (sensitivity) vs false positive rate (1-specificity) at different decision thresholds. This allows us to calculate the area under the curve (AUC) to estimate model performance. An AUC of 0.5 indicates a model that performs as well as a coin flip (50/50)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb73c5f-ce83-46d4-a0b3-48091b12d4c8",
   "metadata": {},
   "source": [
    "The use of the `GridSearchCV()` function, automatically refits the best trained/tuned model using all of our data.\n",
    "\n",
    "Thus, we can use the fitted object to evaluate some performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7109aa7d-c3f3-477b-aa7f-50bc3b5c00f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the predicted probabilities on all of the training data\n",
    "y_proba = fit_lr_grid_search.predict_proba(X_train)[:,1] \n",
    "\n",
    "# calculate the area under the curve (AUC)\n",
    "auc = roc_auc_score(y_train, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0bbd8-4dbf-4920-83ac-560466ed4752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate false positive rate and the true positive rate in order to construct a receiver operating curve (ROC)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_proba, pos_label=1)\n",
    "\n",
    "plt.title('ROC (Train)')\n",
    "ax1 = plt.plot(fpr, tpr, 'b', label = '%s AUC = %0.4f' % ('', auc), linewidth=2)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a4a79-b270-44de-b9da-733b6fe8ae34",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve\n",
    "\n",
    "The curve plots the prcesion and recall at different decision thresholds. Recall or the true positive rate tells us how well the model correctly identifies all positive cases, whereas precision estimates the proportion of cases the model identified as positive that were truely positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305cc2e0-125a-48f4-b62a-bfbd77c03144",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = fit_lr_grid_search.predict_proba(X_train)[:,1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_proba, pos_label=1, sample_weight=None)\n",
    "avg_p = average_precision_score(y_train, y_proba, pos_label=1, sample_weight=None)\n",
    "\n",
    "plt.title('Precision-Recall Curve (Train)')\n",
    "ax1= plt.plot(precision, recall, 'b', label = '%s AP = %0.3f' % ('', avg_p), linewidth=2)\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e43cba-127e-4d45-a450-870a2335c9b3",
   "metadata": {},
   "source": [
    "## Evaluation on Held-out Test Set\n",
    "\n",
    "Let's see how well our trained model does on data it has never seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51af1a-b66e-42a5-87ba-6f44766252f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../challenge2012/data/PhysionetChallenge2012-set-b.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5be10-f40e-4ed6-ac60-288e56ebb002",
   "metadata": {},
   "source": [
    "We have to make sure our data is structured simiarly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba04e40-c34c-4f2e-890b-91111000e0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = test[['Age', 'Gender', 'Weight', 'CCU', 'CSRU', 'SICU', 'GCS_first',\n",
    "       'Glucose_first', 'HR_first', 'NIDiasABP_first', 'NIMAP_first',\n",
    "       'NISysABP_first', 'Temp_first', 'BUN_first', 'Creatinine_first',\n",
    "       'HCO3_first', 'HCT_first', 'K_first', 'Mg_first', 'Na_first',\n",
    "       'PaCO2_first', 'PaO2_first', 'Platelets_first', 'WBC_first',\n",
    "       'pH_first']]\n",
    "\n",
    "# we will drop the small handful of patients without a gender\n",
    "X_test = X_test.dropna(subset=['Gender'])\n",
    "\n",
    "## format the y_test variable\n",
    "y_test = test.copy()\n",
    "\n",
    "# merge with the index for train to remove the handful of patients without gender\n",
    "y_test = pd.merge(y_test['In-hospital_death'], X_test,\n",
    "                  left_index=True, \n",
    "                  right_index=True)\n",
    "\n",
    "# select only the outcome of interest\n",
    "y_test = y_test['In-hospital_death']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505925f-0580-4641-862e-622b3cba2993",
   "metadata": {},
   "source": [
    "### Final Test-set Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17adc363-b250-426b-8466-421f0b1a5b8d",
   "metadata": {},
   "source": [
    "#### Confusion Matrix (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19daf447-0979-4968-92e3-f9be3f1494fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted classes using default 0.5 threshold\n",
    "y_hat_test = fit_lr_grid_search.predict(X_test) \n",
    "\n",
    "print(\"classification report (Test):\\n \", classification_report(y_test, y_hat_test, digits=3))\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_hat_test)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964dbf93-1011-4acd-95c2-d77d69b43446",
   "metadata": {},
   "source": [
    "#### Receiver Operating Characteristic Curve (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996bdc9c-c3e9-43ef-ad62-2c0dc17bda14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the predicted probabilities on all of the training data\n",
    "y_proba_test = fit_lr_grid_search.predict_proba(X_test)[:,1] \n",
    "\n",
    "# calculate the area under the curve (AUC)\n",
    "auc = roc_auc_score(y_test, y_proba_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54736fc-7cd0-43d4-8217-e0320f960f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate false positive rate and the true positive rate in order to construct a receiver operating curve (ROC)\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_proba_test, pos_label=1)\n",
    "\n",
    "plt.title('ROC (Test)')\n",
    "ax1 = plt.plot(fpr_test, tpr_test, 'b', label = '%s AUC = %0.4f' % ('', auc), linewidth=2)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ba96e-90da-43cf-b079-abb7c4279085",
   "metadata": {},
   "source": [
    "#### Precision-Recall Curve (Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27539a6-cda2-4df4-bd22-1d5d5e6a6b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_test, pos_label=1, sample_weight=None)\n",
    "avg_p_test = average_precision_score(y_test, y_proba_test, pos_label=1, sample_weight=None)\n",
    "\n",
    "plt.title('Precision-Recall Curve (Test)')\n",
    "ax1= plt.plot(precision, recall, 'b', label = '%s AP = %0.3f' % ('', avg_p_test), linewidth=2)\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f27527-1310-4df5-a692-f67778665289",
   "metadata": {},
   "source": [
    "### **Model Evaluation on held-out test-set**\n",
    "\n",
    "- metrics for evaluation\n",
    "\n",
    "- think about the clinical outcome of interest - what evaluation metric makes the most sense "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ee219-5890-48ec-b97d-9aabfc6b34ca",
   "metadata": {},
   "source": [
    "### **Error Analysis for Model Diagnosis**\n",
    "\n",
    "- which patients are we getting wrong/confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdabf91-7a3d-4eaf-8bbb-b79da4f83dc0",
   "metadata": {},
   "source": [
    "#### **Questions to Consider:**\n",
    "\n",
    "* Which features seem most predictive for classification? How do you know?\n",
    "\n",
    "* What metrics should we prioritize when evaluating this model?\n",
    "\n",
    "* How might the following affect model performance:\n",
    "\n",
    "    * Missing values\n",
    "\n",
    "    * Class imbalance\n",
    "    \n",
    "* What other types of experiments could we perform with this dataset? Is mortality prediction the best use case? Why or why not?\n",
    "\n",
    "* Are there any downsides to grid search? What might be some other ways to train/tune the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb57df0-2038-4708-9f3e-80513e74ffff",
   "metadata": {},
   "source": [
    "## **Additional Experiments**\n",
    "\n",
    "---\n",
    "\n",
    "**What happens if we train our model on a smaller dataset of only 500 patients with **X** having the positive class?**\n",
    "\n",
    "\n",
    "**Rather than imputing, how does model performance change if we drop all patients with at least 1 missing value?**\n",
    "\n",
    "- Could there be any biases when we drop patients with 'missing' values\n",
    "\n",
    "**Does the model perform better if we specify patients who have missing values (i.e. create an additional column)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e52f7",
   "metadata": {},
   "source": [
    "## Random Forest Algorithm\n",
    "\n",
    "<img src=\"lecture_images/decision_tree.png\" alt=\"title\" width=\"400\" align=\"left\">\n",
    "\n",
    "* Random forest is an esembling algorithm that learns from generating multiple (hundreds-thousands) of decision trees\n",
    "* Randomly selects a subset of features when growing each tree \n",
    "* This approach allows us to average the predictions of the \"crowd\"\n",
    "\n",
    "<b></b>\n",
    "* Many **hyperparameters** that can be tuned during training:\n",
    "<br></br>\n",
    "* `n_estimators`: Number of trees\n",
    "* `max_features`: Number of featues to be considered at each split \n",
    "* `max_depth`: Maximum number of levels in tree (can help regularize the tree to prevent overfitting)\n",
    "* `min_samples_split`: Minimum samples to split a node \n",
    "* `min_samples_leaf`: Minimum number of samples required at each leaf node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### tuning RF hyperparameters\n",
    "# Number of trees in random forest\n",
    "n_estimators = [100, 300, 500, 1000]\n",
    "# Number of features to consider at every split\n",
    "max_features = [2, 5]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5, 8]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [10, 15]\n",
    "\n",
    "# create grid of hypterparameter settings\n",
    "# we will permute through this grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6a97a",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e006f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(criterion='entropy', random_state=12345, class_weight = 'balanced')\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator = rf,\n",
    "                           param_grid = param_grid,\n",
    "                           cv = 5,\n",
    "                           scoring = 'roc_auc',\n",
    "                           return_train_score = False,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a885232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the cv_results_ dictionary\n",
    "cv_results_rf = grid_search.cv_results_\n",
    "\n",
    "# Create a DataFrame from cv_results_\n",
    "rf_results_df = pd.DataFrame(cv_results_rf)\n",
    "\n",
    "# Extract the row corresponding to the best index\n",
    "rf_best_row_df = rf_results_df.loc[[grid_search.best_index_]]  # Use double brackets to keep it as a DataFrame\n",
    "rf_best_row_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96bcfe",
   "metadata": {},
   "source": [
    "### Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_rf.predict(X_train)\n",
    "y_prob = best_rf.predict_proba(X_train)[:,1]\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "precision = precision_score(y_train, y_pred)\n",
    "recall = recall_score(y_train, y_pred)\n",
    "auroc = roc_auc_score(y_train, y_prob)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"AUC:\", auroc)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9befb41a",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review feature importances on the training set\n",
    "feature_importances = pd.Series(best_rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot a simple bar chart\n",
    "feature_importances.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60105593",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = best_rf.predict(X_train) # predicted classes using default 0.5 threshold\n",
    "y_proba = best_rf.predict_proba(X_train)[:,1] #predicted probabilities\n",
    "auc=roc_auc_score(y_train, y_proba)\n",
    "loss= log_loss(y_train, y_hat)\n",
    "\n",
    "print ('the AUC is: {:0.3f}'.format(auc))\n",
    "print ('the logloss is: {:0.3f}'.format(loss))\n",
    "print(\"classification report:\\n \", classification_report(y_train,y_hat, digits=3))\n",
    "print(\"confusion matrix:\\n \")\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_train, y_hat)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53b08d",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_train, y_proba, pos_label=1)\n",
    "\n",
    "plt.title('ROC curve')\n",
    "ax1 = plt.plot(fpr, tpr, 'b', label = '%s AUC = %0.3f' % ('', auc), linewidth=2)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed8623",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eaecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = best_rf.predict_proba(X_train)[:,1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_proba, pos_label=1, sample_weight=None)\n",
    "avg_p = average_precision_score(y_train, y_proba, pos_label=1, sample_weight=None)\n",
    "\n",
    "plt.title('Precision-Recall curve')\n",
    "ax1= plt.plot(precision, recall, 'b', label = '%s AP = %0.3f' % ('', avg_p), linewidth=2)\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2919b9a-ffdb-4f6f-8ed2-3c164af58c34",
   "metadata": {},
   "source": [
    "### Need to Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e3089",
   "metadata": {},
   "source": [
    "### Evaluation of Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b3e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = best_rf.predict(X_test) # predicted classes using default 0.5 threshold\n",
    "y_proba = best_rf.predict_proba(X_test)[:,1] #predicted probabilities\n",
    "auc=roc_auc_score(y_test, y_proba)\n",
    "loss= log_loss(y_test, y_hat)\n",
    "\n",
    "print ('the AUC is: {:0.3f}'.format(auc))\n",
    "print ('the logloss is: {:0.3f}'.format(loss))\n",
    "print(\"classification report:\\n \", classification_report(y_test,y_hat, digits=3))\n",
    "print(\"confusion matrix:\\n \")\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139f850",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99806ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba, pos_label=1)\n",
    "\n",
    "plt.title('ROC curve')\n",
    "ax1 = plt.plot(fpr, tpr, 'b', label = '%s AUC = %0.3f' % ('', auc), linewidth=2)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a24bad",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = best_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba, pos_label=1, sample_weight=None)\n",
    "avg_p = average_precision_score(y_test, y_proba, pos_label=1, sample_weight=None)\n",
    "\n",
    "plt.title('Precision-Recall curve')\n",
    "ax1= plt.plot(precision, recall, 'b', label = '%s AP = %0.3f' % ('', avg_p), linewidth=2)\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "medAI_edu (Python 3.13.1)",
   "language": "python",
   "name": "medai_edu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
